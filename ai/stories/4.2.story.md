Title: Epic 4, Story 2: Create Public Chat Proxy Endpoint

Goal: To create the public endpoint that securely proxies chat messages from the widget to the OpenAI API.

User Story: As the chat widget, when a user sends a message, I need to send it to the FitBot API, which will then securely forward it to OpenAI and stream the response back to me.

Requirements:

Create a POST /widget/chat endpoint protected by the ApiKeyAuthGuard.

The endpoint should accept a JSON body with { message, history }.

The controller must:

Retrieve the user's configuration (including faqText and encrypted openAiApiKey) via the ApiKeyAuthGuard.

Decrypt the openAiApiKey.

Construct a prompt for the OpenAI API, including the user's faqText as context and the end-user's message.

Make a request to the OpenAI Completions API.

Stream the response from OpenAI directly back to the client.

Acceptance Criteria:

The endpoint is protected and fails unauthorized requests.

A valid request successfully proxies a message to a mock OpenAI service.

The faqText from the user's configuration is used in the prompt sent to OpenAI.

The user's openAiApiKey is used for the OpenAI request and is never exposed to the client.

The response from the OpenAI service is streamed back to the client.

Technical Context:

Endpoint: POST /widget/chat

Authentication: ApiKeyAuthGuard

External Service: OpenAI API

Key Services: EncryptionService

Streaming: Use NestJS's support for Server-Sent Events (SSE) or a streaming library.

Testing Requirements:

This is difficult to e2e test directly. Create integration tests that use a mock OpenAI client to verify the service constructs the correct prompt and handles the data flow properly.